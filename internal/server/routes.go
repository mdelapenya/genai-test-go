package server

import (
	"genai-test-go/internal/ai"
	"log"
	"net/http"
	"strconv"
	"strings"

	"github.com/gofiber/fiber/v2"
	"github.com/tmc/langchaingo/chains"
	"github.com/tmc/langchaingo/llms"
)

const (
	question string = "Since which Testcontainers for Go version is the Grafana LGTM module available?"

	// Using must/should is important
	reference = `- Answer must not mention any other module
- Answer must mention the version of Testcontainers for Go, which is v0.33.0
- Answer must be less than 5 sentences`
)

func (s *FiberServer) RegisterFiberRoutes() {
	s.App.Get("/", s.HelloWorldHandler)

	openAIApis := s.App.Group("/openai")
	openAIApis.Add(http.MethodGet, "/rag", s.OpenAIRagHandler)
	openAIApis.Add(http.MethodGet, "/llm", s.OpenAILLHandler)

	ollamaApis := s.App.Group("/ollama")
	ollamaApis.Add(http.MethodGet, "/rag", s.OllamaRagHandler)
	ollamaApis.Add(http.MethodGet, "/llm", s.OllamaLLHandler)
}

func (s *FiberServer) HelloWorldHandler(c *fiber.Ctx) error {
	resp := fiber.Map{
		"message": "Hello World",
	}

	return c.JSON(resp)
}

func parseTemperature(c *fiber.Ctx) float64 {
	var temperature float64
	// read it from the query string
	if temp := c.Query("t"); temp != "" {
		t, err := strconv.ParseFloat(temp, 64)
		if err != nil {
			return 0.0
		}
		temperature = t
	}

	if temperature > 1.0 {
		temperature = 1.0
	} else if temperature < 0.0 {
		temperature = 0.0
	}

	return temperature
}

func llmHandler(c *fiber.Ctx, llm llms.Model) error {
	response, err := llm.GenerateContent(c.Context(), []llms.MessageContent{
		llms.TextParts(llms.ChatMessageTypeHuman, question),
	}, llms.WithTemperature(parseTemperature(c)))
	if err != nil {
		log.Fatal(err)
	}

	answer := strings.ReplaceAll(response.Choices[0].Content, "\"", "'")

	// even though we could be using Ollama, we can still use the same evaluator, GPT4,
	// which is the smartest model we have. As a consequence, the evaluator will
	// provide a better answer than the one generated by Ollama.

	evaluator := ai.NewEvaluator(server.llm)
	aiResp, err := evaluator.Evaluate(c.Context(), question, answer, reference)
	if err != nil {
		return c.Status(fiber.StatusInternalServerError).JSON(fiber.Map{
			"error": err.Error(),
		})
	}

	resp := fiber.Map{
		"evaluator": aiResp,
		"answer":    answer,
		"question":  question,
		"reference": reference,
	}

	return c.JSON(resp)
}

func ragHandler(c *fiber.Ctx, cr chains.ConversationalRetrievalQA) error {
	answer, err := chains.Run(c.Context(), cr, question, chains.WithTemperature(parseTemperature(c)))
	if err != nil {
		return c.Status(fiber.StatusInternalServerError).JSON(fiber.Map{
			"error": err.Error(),
		})
	}

	answer = strings.ReplaceAll(answer, "\"", "'")

	// even though we could be using Ollama, we can still use the same evaluator, GPT4,
	// which is the smartest model we have. As a consequence, the evaluator will
	// provide a better answer than the one generated by Ollama.

	evaluator := ai.NewEvaluator(server.llm)
	aiResp, err := evaluator.Evaluate(c.Context(), question, answer, reference)
	if err != nil {
		return c.Status(fiber.StatusInternalServerError).JSON(fiber.Map{
			"error": err.Error(),
		})
	}

	resp := fiber.Map{
		"evaluator": aiResp,
		"answer":    answer,
		"question":  question,
		"reference": reference,
	}

	return c.JSON(resp)
}
