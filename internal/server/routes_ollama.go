package server

import (
	"genai-test-go/internal/ai"
	"log"
	"strings"

	"github.com/gofiber/fiber/v2"
	"github.com/tmc/langchaingo/chains"
	"github.com/tmc/langchaingo/llms"
)

func (s *FiberServer) OllamaLLHandler(c *fiber.Ctx) error {
	response, err := s.ollamaModel.GenerateContent(c.Context(), []llms.MessageContent{
		llms.TextParts(llms.ChatMessageTypeHuman, question),
	}, llms.WithTemperature(parseTemperature(c)))
	if err != nil {
		log.Fatal(err)
	}

	answer := strings.ReplaceAll(response.Choices[0].Content, "\"", "'")

	// even though we are using Ollama, we can still use the same evaluator, GPT4,
	// which is the smartest model we have. As a consequence, the evaluator will
	// provide a better answer than the one generated by Ollama.
	evaluator := ai.NewEvaluator(server.llm)
	aiResp, err := evaluator.Evaluate(c.Context(), question, answer, reference)
	if err != nil {
		return c.Status(fiber.StatusInternalServerError).JSON(fiber.Map{
			"error": err.Error(),
		})
	}

	resp := fiber.Map{
		"evaluator": aiResp,
		"answer":    answer,
		"question":  question,
		"reference": reference,
	}

	return c.JSON(resp)
}

func (s *FiberServer) OllamaRagHandler(c *fiber.Ctx) error {
	answer, err := chains.Run(c.Context(), s.ollamaConversationalRetrieval, question, chains.WithTemperature(parseTemperature(c)))
	if err != nil {
		return c.Status(fiber.StatusInternalServerError).JSON(fiber.Map{
			"error": err.Error(),
		})
	}

	answer = strings.ReplaceAll(answer, "\"", "'")

	evaluator := ai.NewEvaluator(server.llm)
	aiResp, err := evaluator.Evaluate(c.Context(), question, answer, reference)
	if err != nil {
		return c.Status(fiber.StatusInternalServerError).JSON(fiber.Map{
			"error": err.Error(),
		})
	}

	resp := fiber.Map{
		"evaluator": aiResp,
		"answer":    answer,
		"question":  question,
		"reference": reference,
	}

	return c.JSON(resp)
}
